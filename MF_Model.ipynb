{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('./char_embedding.pkl', 'rb')\n",
    "dict_char = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open ('./dataset/item_index_withEN.pkl', 'rb')\n",
    "item_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open ('./dataset/name_index_withEN.pkl', 'rb')\n",
    "user_index = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mat = np.load('dataset/user_item_withEN_mat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_char 21592\n",
      "user_index 15251\n",
      "item_index 32\n",
      "rating_mat (15251, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"dict_char\", len(dict_char))\n",
    "print(\"user_index\", len(user_index))\n",
    "print(\"item_index\", len(item_index))\n",
    "print(\"rating_mat\", rating_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#dict_char['沈']\n",
    "#user_list.remove('')\n",
    "k, v = random.choice(list(dict_char.items()))\n",
    "print(type(v))\n",
    "#print(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15250\n",
      "missing char 190\n",
      "user_mat (15250, 256)\n",
      "user_con_mat (15250, 768)\n",
      "remain 15250\n",
      "remain (15250, 32)\n"
     ]
    }
   ],
   "source": [
    "user_list = list(user_index.keys())\n",
    "print(len(user_list))\n",
    "user_mat = []\n",
    "user_con_mat = []\n",
    "user_emb = []\n",
    "con_emb = []\n",
    "count = 0\n",
    "\n",
    "for user in user_list:\n",
    "    temp = []\n",
    "    for char in user:\n",
    "        try:\n",
    "            temp.append(dict_char[char])\n",
    "        except:\n",
    "            count += 1\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "    #print(len(temp))\n",
    "    if len(temp) == 2:\n",
    "        # k, v = random.choice(list(dict_char.items()))\n",
    "        user_emb = np.mean([temp[0], temp[1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[1]))\n",
    "    elif len(temp) == 3:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    elif len(temp) == 4:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2], temp[-1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    else:\n",
    "        print(\"index\", user_list.index(user))\n",
    "        #rating_mat = np.delete(rating_mat, [user_list.index(user)], axis=0)\n",
    "        user_list.remove(user)\n",
    "        del user_index[user]\n",
    "        print(len(temp))\n",
    "    user_con_mat.append(con_emb)\n",
    "    user_mat.append(user_emb)\n",
    "\n",
    "user_mat = np.array(user_mat)\n",
    "user_con_mat = np.array(user_con_mat)\n",
    "print(\"missing char\", count)\n",
    "print(\"user_mat\", user_mat.shape)\n",
    "print(\"user_con_mat\", user_con_mat.shape)\n",
    "print(\"remain\", len(user_list))\n",
    "print(\"remain\", rating_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25516173  0.02200902 -0.6367982  ...  0.27963406  0.06052778\n",
      "  -0.2570803 ]\n",
      " [-0.00804286 -0.43483147 -0.4136071  ...  0.5910477   0.11055978\n",
      "  -0.76872826]\n",
      " [-0.18077803 -0.28984937 -0.42697546 ...  0.38626578  0.0096873\n",
      "  -0.5621365 ]\n",
      " ...\n",
      " [ 0.31248116 -0.27425033  0.06705994 ...  0.5632503   0.6493718\n",
      "  -0.25115702]\n",
      " [-0.59236026  0.25725064  0.44351003 ...  0.8128228  -0.12497162\n",
      "  -0.08346231]\n",
      " [-0.06863866  0.05283417  0.21988375 ...  0.6965713   0.10992792\n",
      "  -0.58061045]]\n"
     ]
    }
   ],
   "source": [
    "#item_shape = (256, 32)\n",
    "#item_mat = np.random.rand(32, 256)\n",
    "print(user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = 15250\n",
    "num_item = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Embedding,Reshape,Input,Dot, Dense, dot, Lambda\n",
    "from keras.models import load_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(num_user, num_item, k):\n",
    "#     input_user = Input(shape=(256,),dtype=\"float32\")\n",
    "#     #model_uer = Embedding(num_user+1,k,input_length = 1)(input_uer)\n",
    "#     model_user = Dense(k, activation='relu')(input_user)\n",
    "#     model_user = Reshape((k,))(model_user)\n",
    "    \n",
    "#     input_item = Input(shape=(32,),dtype=\"float32\")\n",
    "#     model_item  = Embedding(num_item+1,k,input_length = 32)(input_item)\n",
    "#     model_item = Dense(256, activation='relu')(model_item)\n",
    "#     model_item = Reshape((256,))(model_item)\n",
    "    \n",
    "#     out = Dot(1)([model_user,model_item])\n",
    "#     model = Model(inputs=[input_user,model_item], outputs=out)\n",
    "#     model.compile(loss='mse', optimizer='Adam')\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recmand_model(num_user,num_item,k):\n",
    "    input_uer = Input(shape=(768, ),dtype=\"float32\")\n",
    "    model_uer = Dense(256, activation='relu')(input_uer)\n",
    "    model_uer = Dense(128, activation='relu')(model_uer)\n",
    "    model_uer = Dense(k, activation='relu')(model_uer)\n",
    "    \n",
    "    \n",
    "    input_item = Input(shape=(1,), dtype=\"float32\")\n",
    "    model_item  = Embedding(num_item, k, input_length = 1)(input_item)\n",
    "    model_item = Reshape((k,))(model_item)\n",
    "    \n",
    "    out = Lambda(lambda x: K.sum(x[0]*x[1], axis=-1, keepdims=True))([model_uer, model_item])\n",
    "    model = Model(inputs=[input_uer, input_item], outputs=out)\n",
    "    model.compile(loss='mse', optimizer='Adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 768)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 64)        2048        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 240,064\n",
      "Trainable params: 240,064\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Recmand_model(num_user, num_item, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "test_user = np.random.random((10, 256))\n",
    "test_item = np.random.randint(0,32, (10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98150853 0.70062474 0.13726937 ... 0.1607718  0.9482678  0.28295064]\n",
      " [0.18701608 0.67779834 0.75381696 ... 0.41377164 0.2374937  0.91917589]\n",
      " [0.91128038 0.60146085 0.19532374 ... 0.60367989 0.0329808  0.08523302]\n",
      " ...\n",
      " [0.29103944 0.46783427 0.38640484 ... 0.73774472 0.89547298 0.70397321]\n",
      " [0.58210377 0.44913554 0.13051729 ... 0.24020321 0.72692221 0.16707797]\n",
      " [0.77469958 0.50775915 0.56931871 ... 0.45033486 0.25050769 0.73255657]]\n"
     ]
    }
   ],
   "source": [
    "print(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27 19  0 12 19 29  2 30  2 28]\n"
     ]
    }
   ],
   "source": [
    "print(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test predict\n",
    "#model.predict([np.random.random((10, 256)), np.random.randint(0,32, (10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25516173  0.02200902 -0.6367982  ...  0.27963406  0.06052778\n",
      "  -0.2570803 ]\n",
      " [-0.00804286 -0.43483147 -0.4136071  ...  0.5910477   0.11055978\n",
      "  -0.76872826]\n",
      " [-0.18077803 -0.28984937 -0.42697546 ...  0.38626578  0.0096873\n",
      "  -0.5621365 ]\n",
      " ...\n",
      " [ 0.31248116 -0.27425033  0.06705994 ...  0.5632503   0.6493718\n",
      "  -0.25115702]\n",
      " [-0.59236026  0.25725064  0.44351003 ...  0.8128228  -0.12497162\n",
      "  -0.08346231]\n",
      " [-0.06863866  0.05283417  0.21988375 ...  0.6965713   0.10992792\n",
      "  -0.58061045]]\n"
     ]
    }
   ],
   "source": [
    "#print(user_con_mat)\n",
    "print(user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index = 0\n",
    "item_index = 0\n",
    "train_x = []\n",
    "train_user = []\n",
    "train_item = []\n",
    "\n",
    "for u in rating_mat:\n",
    "    for i in u:\n",
    "        if i == 1:\n",
    "            # train_x.append([user_mat[user_index], [item_index]])\n",
    "            u_list = user_con_mat[user_index].tolist()\n",
    "            train_user.append(u_list)\n",
    "            #train_user.append(user_mat[user_index].tolist())\n",
    "            #print(user_mat[user_index].tolist())\n",
    "            train_item.append(item_index)\n",
    "        item_index += 1\n",
    "    user_index += 1\n",
    "    item_index = 0\n",
    "train_x = [np.array(train_user), np.array(train_item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.74557173, -0.02800325, -0.72160846, ...,  0.50239176,\n",
      "         0.42607114, -0.61805236],\n",
      "       [-0.74557173, -0.02800325, -0.72160846, ...,  0.50239176,\n",
      "         0.42607114, -0.61805236],\n",
      "       [-0.74557173, -0.02800325, -0.72160846, ...,  0.50239176,\n",
      "         0.42607114, -0.61805236],\n",
      "       ...,\n",
      "       [-0.02675579, -0.89014792,  0.57638884, ...,  0.35549855,\n",
      "         0.82068586,  0.22624786],\n",
      "       [-0.48385873,  0.44448829,  0.65895814, ...,  1.33896124,\n",
      "        -0.20171902, -0.70200801],\n",
      "       [ 0.01715979,  0.4324117 ,  0.38358891, ...,  0.98461074,\n",
      "         0.39813921, -0.48753178]]), array([6, 7, 8, ..., 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "#train_x = np.array(train_x)\n",
    "print(train_x)\n",
    "#print(train_x)\n",
    "train_y = [1] * 39911\n",
    "train = [1] * 10\n",
    "#print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31928 samples, validate on 7983 samples\n",
      "Epoch 1/300\n",
      "31928/31928 [==============================] - 3s 101us/step - loss: 0.0094 - val_loss: 0.2577\n",
      "Epoch 2/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 2.4459e-04 - val_loss: 0.2480\n",
      "Epoch 3/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 3.2565e-04 - val_loss: 0.2422\n",
      "Epoch 4/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 2.2590e-04 - val_loss: 0.2383\n",
      "Epoch 5/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 2.6807e-04 - val_loss: 0.2345\n",
      "Epoch 6/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 4.2682e-04 - val_loss: 0.2321\n",
      "Epoch 7/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4091e-04 - val_loss: 0.2300\n",
      "Epoch 8/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 9.8425e-05 - val_loss: 0.2284\n",
      "Epoch 9/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 2.6058e-04 - val_loss: 0.2283\n",
      "Epoch 10/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4328e-04 - val_loss: 0.2255\n",
      "Epoch 11/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 9.3251e-05 - val_loss: 0.2245\n",
      "Epoch 12/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 1.2236e-04 - val_loss: 0.2246\n",
      "Epoch 13/300\n",
      "31928/31928 [==============================] - 3s 81us/step - loss: 1.1922e-04 - val_loss: 0.2219\n",
      "Epoch 14/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 9.0830e-05 - val_loss: 0.2214\n",
      "Epoch 15/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4779e-04 - val_loss: 0.2195\n",
      "Epoch 16/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 5.5591e-05 - val_loss: 0.2188\n",
      "Epoch 17/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 9.2490e-05 - val_loss: 0.2184\n",
      "Epoch 18/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.9881e-05 - val_loss: 0.2178\n",
      "Epoch 19/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.5286e-05 - val_loss: 0.2177\n",
      "Epoch 20/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 8.1097e-05 - val_loss: 0.2176\n",
      "Epoch 21/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 5.3731e-05 - val_loss: 0.2175\n",
      "Epoch 22/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 9.7413e-05 - val_loss: 0.2176\n",
      "Epoch 23/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.4219e-05 - val_loss: 0.2169\n",
      "Epoch 24/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.7978e-05 - val_loss: 0.2169\n",
      "Epoch 25/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.6390e-05 - val_loss: 0.2167\n",
      "Epoch 26/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 2.9404e-05 - val_loss: 0.2167\n",
      "Epoch 27/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 6.3943e-05 - val_loss: 0.2165\n",
      "Epoch 28/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.5123e-05 - val_loss: 0.2166\n",
      "Epoch 29/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 2.9531e-05 - val_loss: 0.2164\n",
      "Epoch 30/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 4.8394e-05 - val_loss: 0.2161\n",
      "Epoch 31/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.7601e-05 - val_loss: 0.2163\n",
      "Epoch 32/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.1707e-05 - val_loss: 0.2163\n",
      "Epoch 33/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 5.5239e-05 - val_loss: 0.2162\n",
      "Epoch 34/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.4358e-05 - val_loss: 0.2158\n",
      "Epoch 35/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 3.3351e-05 - val_loss: 0.2157\n",
      "Epoch 36/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 4.0165e-05 - val_loss: 0.2157\n",
      "Epoch 37/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.0999e-05 - val_loss: 0.2156\n",
      "Epoch 38/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.6905e-05 - val_loss: 0.2152\n",
      "Epoch 39/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.2746e-05 - val_loss: 0.2151\n",
      "Epoch 40/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 4.4271e-05 - val_loss: 0.2150\n",
      "Epoch 41/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.2056e-05 - val_loss: 0.2150\n",
      "Epoch 42/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 3.7979e-05 - val_loss: 0.2149\n",
      "Epoch 43/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.9268e-05 - val_loss: 0.2150\n",
      "Epoch 44/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 3.4535e-06 - val_loss: 0.2150\n",
      "Epoch 45/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.4704e-05 - val_loss: 0.2148\n",
      "Epoch 46/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 8.1984e-06 - val_loss: 0.2148\n",
      "Epoch 47/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.4901e-05 - val_loss: 0.2149\n",
      "Epoch 48/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 3.2961e-05 - val_loss: 0.2148\n",
      "Epoch 49/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 3.3408e-05 - val_loss: 0.2151\n",
      "Epoch 50/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 1.6706e-05 - val_loss: 0.2147\n",
      "Epoch 51/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.6603e-05 - val_loss: 0.2149\n",
      "Epoch 52/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 4.0547e-05 - val_loss: 0.2150\n",
      "Epoch 53/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 7.4006e-06 - val_loss: 0.2151\n",
      "Epoch 54/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 4.3979e-05 - val_loss: 0.2148\n",
      "Epoch 55/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 8.0250e-06 - val_loss: 0.2152\n",
      "Epoch 56/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 2.2999e-05 - val_loss: 0.2152\n",
      "Epoch 57/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 2.1904e-05 - val_loss: 0.2152\n",
      "Epoch 58/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 2.0184e-05 - val_loss: 0.2150\n",
      "Epoch 59/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.4585e-05 - val_loss: 0.2148\n",
      "Epoch 60/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 2.2536e-05 - val_loss: 0.2148\n",
      "Epoch 61/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 2.2398e-05 - val_loss: 0.2149\n",
      "Epoch 62/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.4284e-05 - val_loss: 0.2148\n",
      "Epoch 63/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.5744e-05 - val_loss: 0.2141\n",
      "Epoch 64/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.0966e-05 - val_loss: 0.2140\n",
      "Epoch 65/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.0480e-05 - val_loss: 0.2140\n",
      "Epoch 66/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.5871e-05 - val_loss: 0.2141\n",
      "Epoch 67/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.2157e-05 - val_loss: 0.2140\n",
      "Epoch 68/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 2.6937e-05 - val_loss: 0.2141\n",
      "Epoch 69/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 6.5232e-06 - val_loss: 0.2143\n",
      "Epoch 70/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 3.2544e-05 - val_loss: 0.2141\n",
      "Epoch 71/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.0293e-06 - val_loss: 0.2141\n",
      "Epoch 72/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.1051e-05 - val_loss: 0.2140\n",
      "Epoch 73/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.1995e-05 - val_loss: 0.2140\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.8736e-05 - val_loss: 0.2136\n",
      "Epoch 75/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.5568e-05 - val_loss: 0.2132\n",
      "Epoch 76/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.0181e-05 - val_loss: 0.2132\n",
      "Epoch 77/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 2.2372e-05 - val_loss: 0.2133\n",
      "Epoch 78/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.8501e-05 - val_loss: 0.2133\n",
      "Epoch 79/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.8488e-06 - val_loss: 0.2133\n",
      "Epoch 80/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 2.4849e-05 - val_loss: 0.2134\n",
      "Epoch 81/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 7.7910e-06 - val_loss: 0.2135\n",
      "Epoch 82/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.9655e-05 - val_loss: 0.2134\n",
      "Epoch 83/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.3968e-05 - val_loss: 0.2135\n",
      "Epoch 84/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 8.8858e-06 - val_loss: 0.2135\n",
      "Epoch 85/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.8748e-05 - val_loss: 0.2135\n",
      "Epoch 86/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 9.8278e-06 - val_loss: 0.2136\n",
      "Epoch 87/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.6330e-05 - val_loss: 0.2135\n",
      "Epoch 88/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.3850e-06 - val_loss: 0.2135\n",
      "Epoch 89/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 2.8196e-05 - val_loss: 0.2135\n",
      "Epoch 90/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.0739e-06 - val_loss: 0.2135\n",
      "Epoch 91/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.6349e-05 - val_loss: 0.2136\n",
      "Epoch 92/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.7684e-06 - val_loss: 0.2137\n",
      "Epoch 93/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.6781e-05 - val_loss: 0.2135\n",
      "Epoch 94/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 8.8878e-06 - val_loss: 0.2134\n",
      "Epoch 95/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.2069e-05 - val_loss: 0.2134\n",
      "Epoch 96/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.0832e-05 - val_loss: 0.2134\n",
      "Epoch 97/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.3575e-05 - val_loss: 0.2133\n",
      "Epoch 98/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 2.1187e-05 - val_loss: 0.2134\n",
      "Epoch 99/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.0550e-06 - val_loss: 0.2134\n",
      "Epoch 100/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.9882e-06 - val_loss: 0.2134\n",
      "Epoch 101/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.2398e-05 - val_loss: 0.2133\n",
      "Epoch 102/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 3.9344e-06 - val_loss: 0.2129\n",
      "Epoch 103/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.2790e-05 - val_loss: 0.2126\n",
      "Epoch 104/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.2340e-05 - val_loss: 0.2127\n",
      "Epoch 105/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.2479e-06 - val_loss: 0.2129\n",
      "Epoch 106/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.7993e-05 - val_loss: 0.2129\n",
      "Epoch 107/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 3.9699e-06 - val_loss: 0.2129\n",
      "Epoch 108/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 8.1151e-06 - val_loss: 0.2127\n",
      "Epoch 109/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.8653e-05 - val_loss: 0.2129\n",
      "Epoch 110/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 1.9947e-06 - val_loss: 0.2135\n",
      "Epoch 111/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4131e-05 - val_loss: 0.2136\n",
      "Epoch 112/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 5.9155e-06 - val_loss: 0.2133\n",
      "Epoch 113/300\n",
      "31928/31928 [==============================] - 3s 91us/step - loss: 1.0735e-05 - val_loss: 0.2133\n",
      "Epoch 114/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 6.2154e-06 - val_loss: 0.2132\n",
      "Epoch 115/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 1.0100e-05 - val_loss: 0.2132\n",
      "Epoch 116/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 8.9205e-06 - val_loss: 0.2128\n",
      "Epoch 117/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.1375e-05 - val_loss: 0.2123\n",
      "Epoch 118/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 6.0568e-06 - val_loss: 0.2126\n",
      "Epoch 119/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 9.2104e-06 - val_loss: 0.2126\n",
      "Epoch 120/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 7.8658e-06 - val_loss: 0.2126\n",
      "Epoch 121/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.1699e-05 - val_loss: 0.2126\n",
      "Epoch 122/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 5.5179e-06 - val_loss: 0.2126\n",
      "Epoch 123/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 7.4406e-06 - val_loss: 0.2126\n",
      "Epoch 124/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 5.7345e-06 - val_loss: 0.2126\n",
      "Epoch 125/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.8889e-05 - val_loss: 0.2122\n",
      "Epoch 126/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 3.4295e-07 - val_loss: 0.2119\n",
      "Epoch 127/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 9.2643e-06 - val_loss: 0.2118\n",
      "Epoch 128/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.3911e-06 - val_loss: 0.2117\n",
      "Epoch 129/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 4.8902e-06 - val_loss: 0.2118\n",
      "Epoch 130/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.3389e-05 - val_loss: 0.2117\n",
      "Epoch 131/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.0295e-06 - val_loss: 0.2117\n",
      "Epoch 132/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 7.4735e-06 - val_loss: 0.2117\n",
      "Epoch 133/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.3007e-05 - val_loss: 0.2116\n",
      "Epoch 134/300\n",
      "31928/31928 [==============================] - 3s 82us/step - loss: 5.4957e-06 - val_loss: 0.2116\n",
      "Epoch 135/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 2.2147e-06 - val_loss: 0.2115\n",
      "Epoch 136/300\n",
      "31928/31928 [==============================] - 2s 78us/step - loss: 1.3480e-05 - val_loss: 0.2115\n",
      "Epoch 137/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 2.5972e-06 - val_loss: 0.2115\n",
      "Epoch 138/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 8.9300e-06 - val_loss: 0.2113\n",
      "Epoch 139/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 8.4646e-06 - val_loss: 0.2113\n",
      "Epoch 140/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 3.3288e-06 - val_loss: 0.2113\n",
      "Epoch 141/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 5.8483e-06 - val_loss: 0.2113\n",
      "Epoch 142/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 5.8887e-06 - val_loss: 0.2113\n",
      "Epoch 143/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.1046e-05 - val_loss: 0.2113\n",
      "Epoch 144/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.6857e-06 - val_loss: 0.2113\n",
      "Epoch 145/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.4542e-05 - val_loss: 0.2113\n",
      "Epoch 146/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 6.7149e-07 - val_loss: 0.2113\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 2s 71us/step - loss: 9.1636e-06 - val_loss: 0.2113\n",
      "Epoch 148/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 3.0446e-06 - val_loss: 0.2113\n",
      "Epoch 149/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.3363e-05 - val_loss: 0.2113\n",
      "Epoch 150/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.1368e-06 - val_loss: 0.2113\n",
      "Epoch 151/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.2411e-06 - val_loss: 0.2113\n",
      "Epoch 152/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.7680e-06 - val_loss: 0.2113\n",
      "Epoch 153/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 3.5998e-06 - val_loss: 0.2114\n",
      "Epoch 154/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 9.6191e-06 - val_loss: 0.2114\n",
      "Epoch 155/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 9.2554e-07 - val_loss: 0.2114\n",
      "Epoch 156/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.0939e-05 - val_loss: 0.2114\n",
      "Epoch 157/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 3.1021e-06 - val_loss: 0.2114\n",
      "Epoch 158/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.8327e-06 - val_loss: 0.2114\n",
      "Epoch 159/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 4.6857e-06 - val_loss: 0.2114\n",
      "Epoch 160/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.2523e-06 - val_loss: 0.2114\n",
      "Epoch 161/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 8.9244e-06 - val_loss: 0.2115\n",
      "Epoch 162/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 2.5230e-06 - val_loss: 0.2115\n",
      "Epoch 163/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 7.3553e-06 - val_loss: 0.2115\n",
      "Epoch 164/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 4.4733e-06 - val_loss: 0.2115\n",
      "Epoch 165/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 7.3966e-06 - val_loss: 0.2115\n",
      "Epoch 166/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.2135e-06 - val_loss: 0.2115\n",
      "Epoch 167/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 4.3610e-06 - val_loss: 0.2115\n",
      "Epoch 168/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.1336e-05 - val_loss: 0.2115\n",
      "Epoch 169/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.2376e-07 - val_loss: 0.2115\n",
      "Epoch 170/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 6.2725e-06 - val_loss: 0.2115\n",
      "Epoch 171/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 8.0808e-06 - val_loss: 0.2115\n",
      "Epoch 172/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.9252e-06 - val_loss: 0.2115\n",
      "Epoch 173/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.8294e-06 - val_loss: 0.2115\n",
      "Epoch 174/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.1857e-06 - val_loss: 0.2115\n",
      "Epoch 175/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.0815e-05 - val_loss: 0.2115\n",
      "Epoch 176/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 8.8610e-06 - val_loss: 0.2114\n",
      "Epoch 177/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4346e-07 - val_loss: 0.2114\n",
      "Epoch 178/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.3658e-05 - val_loss: 0.2114\n",
      "Epoch 179/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.6467e-07 - val_loss: 0.2114\n",
      "Epoch 180/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.4392e-06 - val_loss: 0.2115\n",
      "Epoch 181/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 8.7038e-06 - val_loss: 0.2114\n",
      "Epoch 182/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4303e-06 - val_loss: 0.2115\n",
      "Epoch 183/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 9.2758e-06 - val_loss: 0.2115\n",
      "Epoch 184/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 1.7598e-06 - val_loss: 0.2115\n",
      "Epoch 185/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 6.9943e-06 - val_loss: 0.2115\n",
      "Epoch 186/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 3.1209e-06 - val_loss: 0.2115\n",
      "Epoch 187/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 8.3655e-06 - val_loss: 0.2115\n",
      "Epoch 188/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 2.4165e-06 - val_loss: 0.2115\n",
      "Epoch 189/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 2.1526e-05 - val_loss: 0.2119\n",
      "Epoch 190/300\n",
      "31928/31928 [==============================] - 3s 81us/step - loss: 7.2471e-07 - val_loss: 0.2119\n",
      "Epoch 191/300\n",
      "31928/31928 [==============================] - 3s 79us/step - loss: 7.9777e-08 - val_loss: 0.2119\n",
      "Epoch 192/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 4.5671e-06 - val_loss: 0.2119\n",
      "Epoch 193/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 8.5321e-06 - val_loss: 0.2120\n",
      "Epoch 194/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 7.1763e-07 - val_loss: 0.2120\n",
      "Epoch 195/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.4497e-06 - val_loss: 0.2120\n",
      "Epoch 196/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 8.4908e-06 - val_loss: 0.2120\n",
      "Epoch 197/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 6.2553e-06 - val_loss: 0.2120\n",
      "Epoch 198/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.5945e-06 - val_loss: 0.2120\n",
      "Epoch 199/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 4.7443e-06 - val_loss: 0.2120\n",
      "Epoch 200/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 7.2064e-06 - val_loss: 0.2120\n",
      "Epoch 201/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 2.9809e-06 - val_loss: 0.2120\n",
      "Epoch 202/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 3.8228e-06 - val_loss: 0.2120\n",
      "Epoch 203/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 7.3856e-06 - val_loss: 0.2120\n",
      "Epoch 204/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.5778e-06 - val_loss: 0.2120\n",
      "Epoch 205/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 8.0811e-06 - val_loss: 0.2120\n",
      "Epoch 206/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.4905e-06 - val_loss: 0.2120\n",
      "Epoch 207/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 6.3751e-06 - val_loss: 0.2120\n",
      "Epoch 208/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 6.6625e-06 - val_loss: 0.2120\n",
      "Epoch 209/300\n",
      "31928/31928 [==============================] - 3s 87us/step - loss: 3.0282e-06 - val_loss: 0.2121\n",
      "Epoch 210/300\n",
      "31928/31928 [==============================] - 3s 79us/step - loss: 4.7905e-06 - val_loss: 0.2121\n",
      "Epoch 211/300\n",
      "31928/31928 [==============================] - 3s 93us/step - loss: 5.8601e-06 - val_loss: 0.2120\n",
      "Epoch 212/300\n",
      "31928/31928 [==============================] - 3s 81us/step - loss: 3.1449e-06 - val_loss: 0.2120\n",
      "Epoch 213/300\n",
      "31928/31928 [==============================] - 3s 93us/step - loss: 4.2291e-06 - val_loss: 0.2120\n",
      "Epoch 214/300\n",
      "31928/31928 [==============================] - 3s 97us/step - loss: 3.2642e-06 - val_loss: 0.2121\n",
      "Epoch 215/300\n",
      "31928/31928 [==============================] - 3s 96us/step - loss: 3.3159e-06 - val_loss: 0.2121\n",
      "Epoch 216/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 8.6633e-06 - val_loss: 0.2121\n",
      "Epoch 217/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.3906e-06 - val_loss: 0.2121\n",
      "Epoch 218/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.8538e-06 - val_loss: 0.2121\n",
      "Epoch 219/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 1.6555e-06 - val_loss: 0.2121\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 3s 98us/step - loss: 4.6844e-06 - val_loss: 0.2121\n",
      "Epoch 221/300\n",
      "31928/31928 [==============================] - 3s 97us/step - loss: 4.7071e-06 - val_loss: 0.2121\n",
      "Epoch 222/300\n",
      "31928/31928 [==============================] - 3s 93us/step - loss: 4.6096e-06 - val_loss: 0.2125\n",
      "Epoch 223/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 1.4349e-06 - val_loss: 0.2125\n",
      "Epoch 224/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 6.2290e-06 - val_loss: 0.2124\n",
      "Epoch 225/300\n",
      "31928/31928 [==============================] - 3s 81us/step - loss: 1.9674e-06 - val_loss: 0.2124\n",
      "Epoch 226/300\n",
      "31928/31928 [==============================] - 3s 84us/step - loss: 7.4435e-06 - val_loss: 0.2127\n",
      "Epoch 227/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 4.7052e-06 - val_loss: 0.2127\n",
      "Epoch 228/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.1995e-06 - val_loss: 0.2127\n",
      "Epoch 229/300\n",
      "31928/31928 [==============================] - 3s 85us/step - loss: 6.0389e-06 - val_loss: 0.2127\n",
      "Epoch 230/300\n",
      "31928/31928 [==============================] - 3s 93us/step - loss: 9.0065e-07 - val_loss: 0.2127\n",
      "Epoch 231/300\n",
      "31928/31928 [==============================] - 3s 97us/step - loss: 8.6094e-06 - val_loss: 0.2128\n",
      "Epoch 232/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 4.7634e-06 - val_loss: 0.2126\n",
      "Epoch 233/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.5208e-07 - val_loss: 0.2126\n",
      "Epoch 234/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 8.9029e-06 - val_loss: 0.2126\n",
      "Epoch 235/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 3.9556e-07 - val_loss: 0.2126\n",
      "Epoch 236/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 6.1250e-06 - val_loss: 0.2126\n",
      "Epoch 237/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 2.6021e-06 - val_loss: 0.2126\n",
      "Epoch 238/300\n",
      "31928/31928 [==============================] - 2s 77us/step - loss: 2.0060e-06 - val_loss: 0.2126\n",
      "Epoch 239/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 8.1994e-06 - val_loss: 0.2125\n",
      "Epoch 240/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.2964e-06 - val_loss: 0.2127\n",
      "Epoch 241/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 1.8980e-06 - val_loss: 0.2127\n",
      "Epoch 242/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 3.0301e-06 - val_loss: 0.2127\n",
      "Epoch 243/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 7.5491e-06 - val_loss: 0.2127\n",
      "Epoch 244/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.6184e-06 - val_loss: 0.2127\n",
      "Epoch 245/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 4.2635e-06 - val_loss: 0.2128\n",
      "Epoch 246/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.5333e-07 - val_loss: 0.2128\n",
      "Epoch 247/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.1666e-05 - val_loss: 0.2130\n",
      "Epoch 248/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 3.9138e-08 - val_loss: 0.2130\n",
      "Epoch 249/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 5.5665e-06 - val_loss: 0.2130\n",
      "Epoch 250/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.6882e-06 - val_loss: 0.2130\n",
      "Epoch 251/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.8284e-06 - val_loss: 0.2132\n",
      "Epoch 252/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 4.7013e-06 - val_loss: 0.2135\n",
      "Epoch 253/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 4.1854e-06 - val_loss: 0.2140\n",
      "Epoch 254/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 2.5682e-06 - val_loss: 0.2140\n",
      "Epoch 255/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 1.9311e-06 - val_loss: 0.2141\n",
      "Epoch 256/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 4.2356e-06 - val_loss: 0.2141\n",
      "Epoch 257/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.3010e-06 - val_loss: 0.2141\n",
      "Epoch 258/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 4.3855e-06 - val_loss: 0.2141\n",
      "Epoch 259/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 2.2943e-06 - val_loss: 0.2141\n",
      "Epoch 260/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.4670e-06 - val_loss: 0.2137\n",
      "Epoch 261/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 5.2710e-06 - val_loss: 0.2134\n",
      "Epoch 262/300\n",
      "31928/31928 [==============================] - 3s 80us/step - loss: 2.6951e-06 - val_loss: 0.2135\n",
      "Epoch 263/300\n",
      "31928/31928 [==============================] - 2s 76us/step - loss: 3.0436e-06 - val_loss: 0.2135\n",
      "Epoch 264/300\n",
      "31928/31928 [==============================] - 3s 86us/step - loss: 2.9094e-06 - val_loss: 0.2136\n",
      "Epoch 265/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.5828e-06 - val_loss: 0.2136\n",
      "Epoch 266/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.8914e-06 - val_loss: 0.2136\n",
      "Epoch 267/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 1.8644e-07 - val_loss: 0.2136\n",
      "Epoch 268/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 4.5248e-06 - val_loss: 0.2137\n",
      "Epoch 269/300\n",
      "31928/31928 [==============================] - 3s 85us/step - loss: 2.1037e-06 - val_loss: 0.2137\n",
      "Epoch 270/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 9.2378e-07 - val_loss: 0.2136\n",
      "Epoch 271/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 5.4946e-06 - val_loss: 0.2136\n",
      "Epoch 272/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 1.5278e-06 - val_loss: 0.2137\n",
      "Epoch 273/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 1.7064e-06 - val_loss: 0.2135\n",
      "Epoch 274/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 5.3411e-06 - val_loss: 0.2136\n",
      "Epoch 275/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 5.1642e-07 - val_loss: 0.2136\n",
      "Epoch 276/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 2.6265e-06 - val_loss: 0.2138\n",
      "Epoch 277/300\n",
      "31928/31928 [==============================] - 2s 73us/step - loss: 2.2008e-06 - val_loss: 0.2138\n",
      "Epoch 278/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 3.0135e-06 - val_loss: 0.2138\n",
      "Epoch 279/300\n",
      "31928/31928 [==============================] - 2s 75us/step - loss: 3.4958e-06 - val_loss: 0.2138\n",
      "Epoch 280/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.3614e-06 - val_loss: 0.2138\n",
      "Epoch 281/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 1.9830e-06 - val_loss: 0.2138\n",
      "Epoch 282/300\n",
      "31928/31928 [==============================] - 2s 72us/step - loss: 3.8011e-06 - val_loss: 0.2139\n",
      "Epoch 283/300\n",
      "31928/31928 [==============================] - 2s 74us/step - loss: 4.9918e-07 - val_loss: 0.2138\n",
      "Epoch 284/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.9287e-06 - val_loss: 0.2139\n",
      "Epoch 285/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 5.2177e-06 - val_loss: 0.2139\n",
      "Epoch 286/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 9.5816e-08 - val_loss: 0.2139\n",
      "Epoch 287/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 1.8019e-06 - val_loss: 0.2139\n",
      "Epoch 288/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.9628e-06 - val_loss: 0.2140\n",
      "Epoch 289/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 2.9418e-06 - val_loss: 0.2140\n",
      "Epoch 290/300\n",
      "31928/31928 [==============================] - 2s 68us/step - loss: 3.7483e-07 - val_loss: 0.2140\n",
      "Epoch 291/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 2.6952e-06 - val_loss: 0.2140\n",
      "Epoch 292/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 5.0322e-06 - val_loss: 0.2140\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 2s 69us/step - loss: 2.1708e-06 - val_loss: 0.2141\n",
      "Epoch 294/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 1.1247e-06 - val_loss: 0.2141\n",
      "Epoch 295/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 1.8035e-06 - val_loss: 0.2141\n",
      "Epoch 296/300\n",
      "31928/31928 [==============================] - 2s 70us/step - loss: 5.0068e-06 - val_loss: 0.2141\n",
      "Epoch 297/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 1.2991e-06 - val_loss: 0.2141\n",
      "Epoch 298/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.1445e-06 - val_loss: 0.2142\n",
      "Epoch 299/300\n",
      "31928/31928 [==============================] - 2s 69us/step - loss: 4.2761e-06 - val_loss: 0.2142\n",
      "Epoch 300/300\n",
      "31928/31928 [==============================] - 2s 71us/step - loss: 1.4631e-07 - val_loss: 0.2142\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size = 64, epochs =300, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_con_300.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_300 = load_model('./my_model_300.h5')\n",
    "model_500 = load_model('./my_model_500.h5')\n",
    "model_con_300 = load_model('./my_model_con_300.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = model.layers[2].get_weights()[0]\n",
    "#embeddings = model.layers[4].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.shape\n",
    "# embed = embeddings.T\n",
    "# print(embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_name_dict = {'CGU':\"長庚大學\", 'CMU':\"中國醫藥大學\", 'CYCU':\"中原大學\", 'FCU':\"逢甲大學\", 'KMU':\"高雄醫學大學\", \n",
    "                'NCCU':\"政治大學\", 'NCHU':\"中興大學\", 'NCKU':\"成功大學\", 'NCTU':\"交通大學\", 'NCUE':\"彰化師範大學\", \n",
    "                'NDHU':\"東華大學\", 'NKNU':\"高雄師範大學\", 'NPTU':\"屏東大學\", 'NSYSU':\"中山大學\", 'NTCU':\"臺中教育大學\", \n",
    "                'NTHU':\"清華大學\", 'NTNU':\"臺灣師範大學\", 'NTU':\"臺灣大學\", 'NTUE':\"臺北教育大學\", 'NUTN':\"臺南大學\",\n",
    "                'SCU':\"東吳大學\", 'THU':\"東海大學\", 'TKU':\"淡江大學\", 'TNUA':\"臺北藝術大學\", 'UTAIPEI':\"臺北市立大學\",\n",
    "                'YM':\"陽明大學\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"馬崇堯\", \"盧秀燕\", \"黃崇明\", \"龍傲天\", \"黃智賢\", \"韓國瑜\", \"林芊吟\"]\n",
    "#name = \"周東誼\"\n",
    "h_name = [\"黃宜軒\", \"黃譯\", \"黃品閎\", \"黃信瑩\", \"黃鈺妤\"]\n",
    "w_name = [\"王文萱\", \"王郁婷\", \"王凱\", \"王昱均\", \"王平婷\"]\n",
    "l_name = [\"林政勳\", \"林聖穎\", \"林文軒\", \"林世威\", \"林嘉蓁\"]\n",
    "c_name = [\"陳芝臻\", \"陳彥融\", \"陳昱澤\", \"陳志軒\", \"陳彥諺\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharEmbed(name):\n",
    "    temp = []\n",
    "    for char in name:\n",
    "        try:\n",
    "            temp.append(dict_char[char])\n",
    "        except:\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "            \n",
    "    if len(temp) == 2:\n",
    "        # k, v = random.choice(list(dict_char.items()))\n",
    "        user_emb = np.mean([temp[0], temp[1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[1]))\n",
    "    elif len(temp) == 3:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    elif len(temp) == 4:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2], temp[-1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "        user_emb = v\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    \n",
    "    #return user_emb\n",
    "    return con_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = []\n",
    "for name in names:\n",
    "    u = getCharEmbed(name)\n",
    "    us.append(u)\n",
    "sc = np.array(range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(us[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict([np.random.random((10, 256)), np.random.randint(0,32, (10))])\n",
    "#np.random.random((10, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'董事': 0,\n",
       " '董事長': 1,\n",
       " '獨立董事': 2,\n",
       " '副董事長': 3,\n",
       " '常務董事': 4,\n",
       " '執行業務股東': 5,\n",
       " 'CGU': 6,\n",
       " 'CMU': 7,\n",
       " 'CYCU': 8,\n",
       " 'FCU': 9,\n",
       " 'KMU': 10,\n",
       " 'NCCU': 11,\n",
       " 'NCHU': 12,\n",
       " 'NCKU': 13,\n",
       " 'NCTU': 14,\n",
       " 'NCUE': 15,\n",
       " 'NDHU': 16,\n",
       " 'NKNU': 17,\n",
       " 'NPTU': 18,\n",
       " 'NSYSU': 19,\n",
       " 'NTCU': 20,\n",
       " 'NTHU': 21,\n",
       " 'NTNU': 22,\n",
       " 'NTU': 23,\n",
       " 'NTUE': 24,\n",
       " 'NUTN': 25,\n",
       " 'SCU': 26,\n",
       " 'THU': 27,\n",
       " 'TKU': 28,\n",
       " 'TNUA': 29,\n",
       " 'UTAIPEI': 30,\n",
       " 'YM': 31}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open ('./dataset/item_index_withEN.pkl', 'rb')\n",
    "item_index_dict = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "item_index_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchool(item_index_dict, score_list):\n",
    "    broad_list = []\n",
    "    for i in range(len(score_list)):\n",
    "        if score_list[i] == 1:\n",
    "            for k, v in item_index_dict.items():\n",
    "                if v == i:\n",
    "                    broad_list.append(k)\n",
    "    return broad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "馬崇堯\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "盧秀燕\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "黃崇明\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
      "龍傲天\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "黃智賢\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "韓國瑜\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "林芊吟\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(us)):\n",
    "    print(names[i])\n",
    "    u = us[i]\n",
    "    score_list = []\n",
    "    for i in range(32):\n",
    "        output = model_con_300.predict([[u], [i]])\n",
    "        if output[0][0] > 0.82:\n",
    "            score_list.append(1)\n",
    "        else:\n",
    "            score_list.append(0)\n",
    "    print(score_list)\n",
    "    output_list = getSchool(item_index_dict, score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = getSchool(item_index_dict, score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(output_list)\n",
    "print(len(output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSchool(name):\n",
    "    count = 0\n",
    "    max_score = 0\n",
    "    max_sc_index = 0\n",
    "    print(name)\n",
    "    u = getCharEmbed(name)\n",
    "    score_list = []\n",
    "    ori_score_dict = {}\n",
    "    for i in range(32):\n",
    "        output = model_con_300.predict([[u], [i]])\n",
    "        #output = model.predict([[u], [i]])\n",
    "        ori_score_dict[output[0][0]] = i\n",
    "        if output[0][0] > max_score:\n",
    "            max_score = output[0][0]\n",
    "            max_sc_index = i\n",
    "        if output[0][0] > 0.82:\n",
    "            score_list.append(1)\n",
    "            count += 1\n",
    "        else:\n",
    "            score_list.append(0)\n",
    "        \n",
    "    ori_score_list = list(ori_score_dict.keys())\n",
    "    ori_score_list.sort(reverse=True)\n",
    "    \n",
    "    print(\"你會上的機率前五高學校或職位有 :\", end=\" \")\n",
    "    # print(ori_score_dict)\n",
    "    # print(list(ori_score_dict.keys()))\n",
    "    for ori in ori_score_list[:5]:\n",
    "        print(list(sc_name_dict.values())[ori_score_dict[ori]-6], end=\" \")\n",
    "    #if count > 0:\n",
    "    print(\"\\n機率最大的是:\", list(sc_name_dict.values())[max_sc_index-6])\n",
    "    \n",
    "    #print(\"你有超過八成機率會上的學校或職位有 :\", count, \"個\")\n",
    "    output_list = getSchool(item_index_dict, score_list)\n",
    "    #print(\"你有超過八成機率會上的學校或職位有 :\", end=\" \")\n",
    "    #for out in output_list:\n",
    "        #print(sc_name_dict[out], end=\" \")\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    \n",
    "        #print(max_sc_index)\n",
    "    #print(\"\\n\")\n",
    "    max_sc_index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林政勳\n",
      "你會上的機率前五高學校或職位有 : 屏東大學 臺南大學 臺北市立大學 政治大學 清華大學 \n",
      "機率最大的是: 屏東大學\n",
      "林聖穎\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 臺南大學 逢甲大學 清華大學 淡江大學 \n",
      "機率最大的是: 臺灣大學\n",
      "林文軒\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 長庚大學 彰化師範大學 \n",
      "機率最大的是: 清華大學\n",
      "林世威\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 屏東大學 東海大學 臺北市立大學 中興大學 \n",
      "機率最大的是: 臺南大學\n",
      "林嘉蓁\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 政治大學 臺中教育大學 逢甲大學 長庚大學 \n",
      "機率最大的是: 臺南大學\n",
      "\n",
      "\n",
      "黃宜軒\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 逢甲大學 政治大學 中山大學 臺北市立大學 \n",
      "機率最大的是: 臺南大學\n",
      "黃譯\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 東華大學 屏東大學 中山大學 彰化師範大學 \n",
      "機率最大的是: 臺南大學\n",
      "黃品閎\n",
      "你會上的機率前五高學校或職位有 : 中山大學 臺中教育大學 逢甲大學 臺灣師範大學 長庚大學 \n",
      "機率最大的是: 中山大學\n",
      "黃信瑩\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 臺中教育大學 長庚大學 \n",
      "機率最大的是: 清華大學\n",
      "黃鈺妤\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 東華大學 臺南大學 政治大學 屏東大學 \n",
      "機率最大的是: 臺灣大學\n",
      "\n",
      "\n",
      "王文萱\n",
      "你會上的機率前五高學校或職位有 : 臺中教育大學 臺灣大學 清華大學 逢甲大學 臺北教育大學 \n",
      "機率最大的是: 臺中教育大學\n",
      "王郁婷\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 成功大學 淡江大學 \n",
      "機率最大的是: 清華大學\n",
      "王凱\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 臺灣大學 逢甲大學 中興大學 臺北市立大學 \n",
      "機率最大的是: 臺南大學\n",
      "王昱均\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 屏東大學 清華大學 東海大學 政治大學 \n",
      "機率最大的是: 臺灣大學\n",
      "王平婷\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 長庚大學 臺南大學 \n",
      "機率最大的是: 清華大學\n",
      "\n",
      "\n",
      "陳芝臻\n",
      "你會上的機率前五高學校或職位有 : 政治大學 臺南大學 逢甲大學 臺中教育大學 屏東大學 \n",
      "機率最大的是: 政治大學\n",
      "陳彥融\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 清華大學 淡江大學 長庚大學 東華大學 \n",
      "機率最大的是: 臺灣大學\n",
      "陳昱澤\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 成功大學 中山大學 \n",
      "機率最大的是: 清華大學\n",
      "陳志軒\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 清華大學 屏東大學 淡江大學 長庚大學 \n",
      "機率最大的是: 臺灣大學\n",
      "陳彥諺\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺灣大學 東華大學 政治大學 屏東大學 \n",
      "機率最大的是: 清華大學\n"
     ]
    }
   ],
   "source": [
    "for name in l_name:\n",
    "    predictSchool(name)\n",
    "print(\"\\n\")\n",
    "for name in h_name:\n",
    "    predictSchool(name)\n",
    "print(\"\\n\")\n",
    "for name in w_name:\n",
    "    predictSchool(name)\n",
    "print(\"\\n\")\n",
    "for name in c_name:\n",
    "    predictSchool(name)\n",
    "# for name in names:\n",
    "#     predictSchool(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韓國瑜\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 東海大學 屏東大學 政治大學 臺灣大學 \n",
      "機率最大的是: 臺南大學\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"韓國瑜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黃崇明\n",
      "你會上的機率前五高學校或職位有 : 臺南大學 東海大學 臺灣大學 臺中教育大學 逢甲大學 \n",
      "機率最大的是: 臺南大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 15 個\n",
      "你有超過八成機率會上的學校或職位有 : 長庚大學 逢甲大學 政治大學 交通大學 東華大學 屏東大學 中山大學 臺中教育大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東海大學 臺北藝術大學 臺北市立大學 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"黃崇明\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王麒詳\n",
      "你會上的機率前五高學校或職位有 : 中山大學 臺南大學 臺灣大學 東華大學 逢甲大學 \n",
      "機率最大的是: 中山大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 26 個\n",
      "你有超過八成機率會上的學校或職位有 : 長庚大學 中國醫藥大學 中原大學 逢甲大學 高雄醫學大學 政治大學 中興大學 成功大學 交通大學 彰化師範大學 東華大學 高雄師範大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東吳大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"王麒詳\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "趙偉廷\n",
      "你會上的機率前五高學校或職位有 : 臺灣大學 清華大學 淡江大學 東華大學 長庚大學 \n",
      "機率最大的是: 臺灣大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 0 個\n",
      "你有超過八成機率會上的學校或職位有 : \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"趙偉廷\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黃仁暐\n",
      "你會上的機率前五高學校或職位有 : 清華大學 臺中教育大學 政治大學 長庚大學 臺灣大學 \n",
      "機率最大的是: 清華大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 19 個\n",
      "你有超過八成機率會上的學校或職位有 : 長庚大學 中國醫藥大學 逢甲大學 政治大學 成功大學 彰化師範大學 東華大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣大學 臺北教育大學 臺南大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"黃仁暐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吳昭儀\n",
      "你會上的機率前五高學校或職位有 : 中山大學 臺灣大學 清華大學 東華大學 臺南大學 \n",
      "機率最大的是: 中山大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 5 個\n",
      "你有超過八成機率會上的學校或職位有 : 東華大學 中山大學 清華大學 臺灣大學 臺南大學 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"吳昭儀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林芊吟\n",
      "你會上的機率前五高學校或職位有 : 長庚大學 清華大學 臺灣大學 東吳大學 東華大學 \n",
      "機率最大的是: 長庚大學\n",
      "\n",
      "你有超過八成機率會上的學校或職位有 : 0 個\n",
      "你有超過八成機率會上的學校或職位有 : \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"林芊吟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
