{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('./char_embedding.pkl', 'rb')\n",
    "dict_char = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open ('./dataset/item_index_withEN.pkl', 'rb')\n",
    "item_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open ('./dataset/name_index_withEN.pkl', 'rb')\n",
    "user_index = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mat = np.load('dataset/user_item_withEN_mat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_char 21592\n",
      "user_index 15251\n",
      "item_index 32\n",
      "rating_mat (15251, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"dict_char\", len(dict_char))\n",
    "print(\"user_index\", len(user_index))\n",
    "print(\"item_index\", len(item_index))\n",
    "print(\"rating_mat\", rating_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#dict_char['沈']\n",
    "#user_list.remove('')\n",
    "k, v = random.choice(list(dict_char.items()))\n",
    "print(type(v))\n",
    "#print(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15250\n",
      "missing char 190\n",
      "user_mat (15250, 256)\n",
      "user_con_mat (15250, 768)\n",
      "remain 15250\n",
      "remain (15250, 32)\n"
     ]
    }
   ],
   "source": [
    "user_list = list(user_index.keys())\n",
    "print(len(user_list))\n",
    "user_mat = []\n",
    "user_con_mat = []\n",
    "user_emb = []\n",
    "con_emb = []\n",
    "count = 0\n",
    "\n",
    "for user in user_list:\n",
    "    temp = []\n",
    "    for char in user:\n",
    "        try:\n",
    "            temp.append(dict_char[char])\n",
    "        except:\n",
    "            count += 1\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "    #print(len(temp))\n",
    "    if len(temp) == 2:\n",
    "        # k, v = random.choice(list(dict_char.items()))\n",
    "        user_emb = np.mean([temp[0], temp[1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[1]))\n",
    "    elif len(temp) == 3:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    elif len(temp) == 4:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2], temp[-1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    else:\n",
    "        print(\"index\", user_list.index(user))\n",
    "        #rating_mat = np.delete(rating_mat, [user_list.index(user)], axis=0)\n",
    "        user_list.remove(user)\n",
    "        del user_index[user]\n",
    "        print(len(temp))\n",
    "    user_con_mat.append(con_emb)\n",
    "    user_mat.append(user_emb)\n",
    "\n",
    "user_mat = np.array(user_mat)\n",
    "user_con_mat = np.array(user_con_mat)\n",
    "print(\"missing char\", count)\n",
    "print(\"user_mat\", user_mat.shape)\n",
    "print(\"user_con_mat\", user_con_mat.shape)\n",
    "print(\"remain\", len(user_list))\n",
    "print(\"remain\", rating_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25516173  0.02200902 -0.6367982  ...  0.27963406  0.06052778\n",
      "  -0.2570803 ]\n",
      " [-0.00804286 -0.43483147 -0.4136071  ...  0.5910477   0.11055978\n",
      "  -0.76872826]\n",
      " [-0.18077803 -0.28984937 -0.42697546 ...  0.38626578  0.0096873\n",
      "  -0.5621365 ]\n",
      " ...\n",
      " [ 0.31248116 -0.27425033  0.06705994 ...  0.5632503   0.6493718\n",
      "  -0.25115702]\n",
      " [-0.59236026  0.25725064  0.44351003 ...  0.8128228  -0.12497162\n",
      "  -0.08346231]\n",
      " [-0.06863866  0.05283417  0.21988375 ...  0.6965713   0.10992792\n",
      "  -0.58061045]]\n"
     ]
    }
   ],
   "source": [
    "#item_shape = (256, 32)\n",
    "#item_mat = np.random.rand(32, 256)\n",
    "print(user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = 15250\n",
    "num_item = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Embedding,Reshape,Input,Dot, Dense, dot, Lambda\n",
    "from keras.models import load_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(num_user, num_item, k):\n",
    "#     input_user = Input(shape=(256,),dtype=\"float32\")\n",
    "#     #model_uer = Embedding(num_user+1,k,input_length = 1)(input_uer)\n",
    "#     model_user = Dense(k, activation='relu')(input_user)\n",
    "#     model_user = Reshape((k,))(model_user)\n",
    "    \n",
    "#     input_item = Input(shape=(32,),dtype=\"float32\")\n",
    "#     model_item  = Embedding(num_item+1,k,input_length = 32)(input_item)\n",
    "#     model_item = Dense(256, activation='relu')(model_item)\n",
    "#     model_item = Reshape((256,))(model_item)\n",
    "    \n",
    "#     out = Dot(1)([model_user,model_item])\n",
    "#     model = Model(inputs=[input_user,model_item], outputs=out)\n",
    "#     model.compile(loss='mse', optimizer='Adam')\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recmand_model(num_user,num_item,k):\n",
    "    input_uer = Input(shape=(256, ),dtype=\"float32\")\n",
    "    model_uer = Dense(128, activation='relu')(input_uer)\n",
    "    model_uer = Dense(128, activation='relu')(model_uer)\n",
    "    model_uer = Dense(k, activation='relu')(model_uer)\n",
    "    \n",
    "    \n",
    "    input_item = Input(shape=(1,), dtype=\"float32\")\n",
    "    model_item  = Embedding(num_item, k, input_length = 1)(input_item)\n",
    "    model_item = Reshape((k,))(model_item)\n",
    "    \n",
    "    out = Lambda(lambda x: K.sum(x[0]*x[1], axis=-1, keepdims=True))([model_uer, model_item])\n",
    "    model = Model(inputs=[input_uer, input_item], outputs=out)\n",
    "    model.compile(loss='mse', optimizer='Adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 64)        2048        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 59,712\n",
      "Trainable params: 59,712\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Recmand_model(num_user, num_item, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "test_user = np.random.random((10, 256))\n",
    "test_item = np.random.randint(0,32, (10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29901757 0.5823561  0.86731756 ... 0.61725474 0.82741774 0.49550487]\n",
      " [0.06286934 0.94492315 0.40991161 ... 0.13180082 0.59067381 0.82368266]\n",
      " [0.67093234 0.32414039 0.13268445 ... 0.29868517 0.8051694  0.44593921]\n",
      " ...\n",
      " [0.49542275 0.75667983 0.03181926 ... 0.38699856 0.78678761 0.67152014]\n",
      " [0.82811759 0.84200775 0.09937715 ... 0.30100663 0.51301124 0.37208237]\n",
      " [0.43832434 0.90023346 0.72115588 ... 0.43404931 0.38658265 0.16685667]]\n"
     ]
    }
   ],
   "source": [
    "print(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 20 21 20  4 27  9  4  4 21]\n"
     ]
    }
   ],
   "source": [
    "print(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test predict\n",
    "#model.predict([np.random.random((10, 256)), np.random.randint(0,32, (10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25516173  0.02200902 -0.6367982  ...  0.27963406  0.06052778\n",
      "  -0.2570803 ]\n",
      " [-0.00804286 -0.43483147 -0.4136071  ...  0.5910477   0.11055978\n",
      "  -0.76872826]\n",
      " [-0.18077803 -0.28984937 -0.42697546 ...  0.38626578  0.0096873\n",
      "  -0.5621365 ]\n",
      " ...\n",
      " [ 0.31248116 -0.27425033  0.06705994 ...  0.5632503   0.6493718\n",
      "  -0.25115702]\n",
      " [-0.59236026  0.25725064  0.44351003 ...  0.8128228  -0.12497162\n",
      "  -0.08346231]\n",
      " [-0.06863866  0.05283417  0.21988375 ...  0.6965713   0.10992792\n",
      "  -0.58061045]]\n"
     ]
    }
   ],
   "source": [
    "#print(user_con_mat)\n",
    "print(user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index = 0\n",
    "item_index = 0\n",
    "train_x = []\n",
    "train_user = []\n",
    "train_item = []\n",
    "\n",
    "for u in rating_mat:\n",
    "    for i in u:\n",
    "        if i == 1:\n",
    "            # train_x.append([user_mat[user_index], [item_index]])\n",
    "            #u_list = user_con_mat[user_index].tolist()\n",
    "            #train_user.append(u_list)\n",
    "            train_user.append(user_mat[user_index].tolist())\n",
    "            #print(user_mat[user_index].tolist())\n",
    "            train_item.append(item_index)\n",
    "        item_index += 1\n",
    "    user_index += 1\n",
    "    item_index = 0\n",
    "train_x = [np.array(train_user), np.array(train_item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.25516173,  0.02200902, -0.6367982 , ...,  0.27963406,\n",
      "         0.06052778, -0.25708029],\n",
      "       [-0.25516173,  0.02200902, -0.6367982 , ...,  0.27963406,\n",
      "         0.06052778, -0.25708029],\n",
      "       [-0.25516173,  0.02200902, -0.6367982 , ...,  0.27963406,\n",
      "         0.06052778, -0.25708029],\n",
      "       ...,\n",
      "       [ 0.31248116, -0.27425033,  0.06705994, ...,  0.5632503 ,\n",
      "         0.6493718 , -0.25115702],\n",
      "       [-0.59236026,  0.25725064,  0.44351003, ...,  0.81282282,\n",
      "        -0.12497162, -0.08346231],\n",
      "       [-0.06863866,  0.05283417,  0.21988375, ...,  0.69657129,\n",
      "         0.10992792, -0.58061045]]), array([6, 7, 8, ..., 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "#train_x = np.array(train_x)\n",
    "print(train_x)\n",
    "#print(train_x)\n",
    "train_y = [1] * 39911\n",
    "train = [1] * 10\n",
    "#print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31928 samples, validate on 7983 samples\n",
      "Epoch 1/300\n",
      "31928/31928 [==============================] - 1s 41us/step - loss: 0.0252 - val_loss: 0.2503\n",
      "Epoch 2/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.3889e-04 - val_loss: 0.2481\n",
      "Epoch 3/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 5.0267e-04 - val_loss: 0.2452\n",
      "Epoch 4/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.0113e-04 - val_loss: 0.2429\n",
      "Epoch 5/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 3.5860e-04 - val_loss: 0.2417\n",
      "Epoch 6/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 3.8398e-04 - val_loss: 0.2401\n",
      "Epoch 7/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.0279e-04 - val_loss: 0.2388\n",
      "Epoch 8/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.5464e-04 - val_loss: 0.2398\n",
      "Epoch 9/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.4879e-04 - val_loss: 0.2381\n",
      "Epoch 10/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.4294e-04 - val_loss: 0.2383\n",
      "Epoch 11/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.9947e-04 - val_loss: 0.2370\n",
      "Epoch 12/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.5399e-04 - val_loss: 0.2367\n",
      "Epoch 13/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.6264e-04 - val_loss: 0.2356\n",
      "Epoch 14/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 1.2343e-04 - val_loss: 0.2345\n",
      "Epoch 15/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.3445e-04 - val_loss: 0.2338\n",
      "Epoch 16/300\n",
      "31928/31928 [==============================] - 1s 38us/step - loss: 1.3519e-04 - val_loss: 0.2329\n",
      "Epoch 17/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 8.5508e-05 - val_loss: 0.2318\n",
      "Epoch 18/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.9840e-05 - val_loss: 0.2306\n",
      "Epoch 19/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 8.3518e-05 - val_loss: 0.2298\n",
      "Epoch 20/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 8.5839e-05 - val_loss: 0.2278\n",
      "Epoch 21/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 5.9973e-05 - val_loss: 0.2274\n",
      "Epoch 22/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 5.6118e-05 - val_loss: 0.2272\n",
      "Epoch 23/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.3483e-05 - val_loss: 0.2263\n",
      "Epoch 24/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 3.4204e-05 - val_loss: 0.2257\n",
      "Epoch 25/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 3.3084e-05 - val_loss: 0.2255\n",
      "Epoch 26/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 3.5182e-05 - val_loss: 0.2240\n",
      "Epoch 27/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 3.2293e-05 - val_loss: 0.2236\n",
      "Epoch 28/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 1.4647e-05 - val_loss: 0.2230\n",
      "Epoch 29/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.9394e-05 - val_loss: 0.2224\n",
      "Epoch 30/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.0399e-05 - val_loss: 0.2224\n",
      "Epoch 31/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.8424e-05 - val_loss: 0.2219\n",
      "Epoch 32/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 4.4268e-06 - val_loss: 0.2218\n",
      "Epoch 33/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 3.0246e-05 - val_loss: 0.2216\n",
      "Epoch 34/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 1.7641e-06 - val_loss: 0.2216\n",
      "Epoch 35/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.0150e-05 - val_loss: 0.2215\n",
      "Epoch 36/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 5.5676e-06 - val_loss: 0.2213\n",
      "Epoch 37/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.7301e-05 - val_loss: 0.2210\n",
      "Epoch 38/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 2.3583e-06 - val_loss: 0.2210\n",
      "Epoch 39/300\n",
      "31928/31928 [==============================] - 1s 39us/step - loss: 1.6598e-05 - val_loss: 0.2207\n",
      "Epoch 40/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 8.6236e-06 - val_loss: 0.2206\n",
      "Epoch 41/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 3.5564e-06 - val_loss: 0.2208\n",
      "Epoch 42/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 1.8161e-05 - val_loss: 0.2200\n",
      "Epoch 43/300\n",
      "31928/31928 [==============================] - 1s 39us/step - loss: 1.5514e-06 - val_loss: 0.2200\n",
      "Epoch 44/300\n",
      "31928/31928 [==============================] - 1s 43us/step - loss: 1.3408e-05 - val_loss: 0.2199\n",
      "Epoch 45/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 6.3888e-06 - val_loss: 0.2198\n",
      "Epoch 46/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 1.3955e-05 - val_loss: 0.2198\n",
      "Epoch 47/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 3.3170e-07 - val_loss: 0.2198\n",
      "Epoch 48/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 1.4930e-05 - val_loss: 0.2199\n",
      "Epoch 49/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 5.5313e-06 - val_loss: 0.2199\n",
      "Epoch 50/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 4.2771e-06 - val_loss: 0.2199\n",
      "Epoch 51/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 1.1768e-05 - val_loss: 0.2197\n",
      "Epoch 52/300\n",
      "31928/31928 [==============================] - 1s 44us/step - loss: 3.3105e-06 - val_loss: 0.2197\n",
      "Epoch 53/300\n",
      "31928/31928 [==============================] - 1s 41us/step - loss: 5.2714e-06 - val_loss: 0.2199\n",
      "Epoch 54/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 1.5457e-05 - val_loss: 0.2201\n",
      "Epoch 55/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.2583e-05 - val_loss: 0.2202\n",
      "Epoch 56/300\n",
      "31928/31928 [==============================] - 1s 39us/step - loss: 1.2130e-06 - val_loss: 0.2202\n",
      "Epoch 57/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 9.4284e-06 - val_loss: 0.2203\n",
      "Epoch 58/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 5.7240e-06 - val_loss: 0.2202\n",
      "Epoch 59/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.7795e-05 - val_loss: 0.2201\n",
      "Epoch 60/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 3.1944e-07 - val_loss: 0.2202\n",
      "Epoch 61/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 6.3899e-06 - val_loss: 0.2201\n",
      "Epoch 62/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 7.2974e-06 - val_loss: 0.2201\n",
      "Epoch 63/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.3535e-05 - val_loss: 0.2199\n",
      "Epoch 64/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 1.1520e-06 - val_loss: 0.2199\n",
      "Epoch 65/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 3.3178e-07 - val_loss: 0.2199\n",
      "Epoch 66/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.3194e-05 - val_loss: 0.2198\n",
      "Epoch 67/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 2.1010e-06 - val_loss: 0.2198\n",
      "Epoch 68/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.0639e-05 - val_loss: 0.2197\n",
      "Epoch 69/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 4.4930e-06 - val_loss: 0.2197\n",
      "Epoch 70/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 7.4477e-06 - val_loss: 0.2197\n",
      "Epoch 71/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 5.0857e-06 - val_loss: 0.2196\n",
      "Epoch 72/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.8836e-06 - val_loss: 0.2196\n",
      "Epoch 73/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 9.9866e-07 - val_loss: 0.2196\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.2588e-05 - val_loss: 0.2196\n",
      "Epoch 75/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.6887e-06 - val_loss: 0.2196\n",
      "Epoch 76/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.2998e-06 - val_loss: 0.2196\n",
      "Epoch 77/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.7803e-06 - val_loss: 0.2197\n",
      "Epoch 78/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.2938e-05 - val_loss: 0.2195\n",
      "Epoch 79/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 9.4335e-08 - val_loss: 0.2195\n",
      "Epoch 80/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.9909e-06 - val_loss: 0.2194\n",
      "Epoch 81/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.7125e-06 - val_loss: 0.2194\n",
      "Epoch 82/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.3839e-06 - val_loss: 0.2195\n",
      "Epoch 83/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.3136e-06 - val_loss: 0.2195\n",
      "Epoch 84/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.1602e-06 - val_loss: 0.2194\n",
      "Epoch 85/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.2264e-06 - val_loss: 0.2194\n",
      "Epoch 86/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 8.8484e-06 - val_loss: 0.2194\n",
      "Epoch 87/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.7356e-06 - val_loss: 0.2194\n",
      "Epoch 88/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.1334e-06 - val_loss: 0.2194\n",
      "Epoch 89/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.8978e-07 - val_loss: 0.2194\n",
      "Epoch 90/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 8.4045e-06 - val_loss: 0.2193\n",
      "Epoch 91/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 3.8396e-06 - val_loss: 0.2193\n",
      "Epoch 92/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 4.3581e-06 - val_loss: 0.2193\n",
      "Epoch 93/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.8500e-06 - val_loss: 0.2193\n",
      "Epoch 94/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.2569e-06 - val_loss: 0.2192\n",
      "Epoch 95/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.5153e-06 - val_loss: 0.2191\n",
      "Epoch 96/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.6808e-06 - val_loss: 0.2192\n",
      "Epoch 97/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.7715e-06 - val_loss: 0.2192\n",
      "Epoch 98/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.0201e-05 - val_loss: 0.2191\n",
      "Epoch 99/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 5.4604e-07 - val_loss: 0.2191\n",
      "Epoch 100/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4974e-06 - val_loss: 0.2191\n",
      "Epoch 101/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.3192e-06 - val_loss: 0.2191\n",
      "Epoch 102/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.4565e-06 - val_loss: 0.2192\n",
      "Epoch 103/300\n",
      "31928/31928 [==============================] - 1s 30us/step - loss: 5.4645e-06 - val_loss: 0.2191\n",
      "Epoch 104/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.2751e-06 - val_loss: 0.2191\n",
      "Epoch 105/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.1139e-07 - val_loss: 0.2191\n",
      "Epoch 106/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.3643e-06 - val_loss: 0.2191\n",
      "Epoch 107/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.5939e-06 - val_loss: 0.2191\n",
      "Epoch 108/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.1928e-07 - val_loss: 0.2191\n",
      "Epoch 109/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.0075e-05 - val_loss: 0.2192\n",
      "Epoch 110/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 6.8615e-07 - val_loss: 0.2192\n",
      "Epoch 111/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.8178e-06 - val_loss: 0.2192\n",
      "Epoch 112/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.7799e-06 - val_loss: 0.2192\n",
      "Epoch 113/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4892e-07 - val_loss: 0.2192\n",
      "Epoch 114/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.5556e-06 - val_loss: 0.2190\n",
      "Epoch 115/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.5893e-06 - val_loss: 0.2190\n",
      "Epoch 116/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 7.3597e-06 - val_loss: 0.2189\n",
      "Epoch 117/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 9.9200e-08 - val_loss: 0.2189\n",
      "Epoch 118/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.0166e-06 - val_loss: 0.2189\n",
      "Epoch 119/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.0711e-06 - val_loss: 0.2189\n",
      "Epoch 120/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.3632e-06 - val_loss: 0.2189\n",
      "Epoch 121/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.8472e-06 - val_loss: 0.2189\n",
      "Epoch 122/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.9372e-06 - val_loss: 0.2189\n",
      "Epoch 123/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.1889e-06 - val_loss: 0.2189\n",
      "Epoch 124/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.1091e-06 - val_loss: 0.2189\n",
      "Epoch 125/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.8302e-06 - val_loss: 0.2189\n",
      "Epoch 126/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.3904e-06 - val_loss: 0.2189\n",
      "Epoch 127/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.0335e-06 - val_loss: 0.2189\n",
      "Epoch 128/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.0195e-06 - val_loss: 0.2188\n",
      "Epoch 129/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.8479e-07 - val_loss: 0.2188\n",
      "Epoch 130/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.9253e-06 - val_loss: 0.2188\n",
      "Epoch 131/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.3495e-06 - val_loss: 0.2188\n",
      "Epoch 132/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.8579e-06 - val_loss: 0.2186\n",
      "Epoch 133/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.9084e-07 - val_loss: 0.2186\n",
      "Epoch 134/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.3449e-06 - val_loss: 0.2185\n",
      "Epoch 135/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.9459e-06 - val_loss: 0.2185\n",
      "Epoch 136/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4066e-06 - val_loss: 0.2184\n",
      "Epoch 137/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.4452e-06 - val_loss: 0.2186\n",
      "Epoch 138/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.5814e-06 - val_loss: 0.2182\n",
      "Epoch 139/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.0602e-07 - val_loss: 0.2182\n",
      "Epoch 140/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.7747e-06 - val_loss: 0.2182\n",
      "Epoch 141/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.3089e-07 - val_loss: 0.2182\n",
      "Epoch 142/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.4342e-06 - val_loss: 0.2180\n",
      "Epoch 143/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.3134e-07 - val_loss: 0.2180\n",
      "Epoch 144/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.5148e-06 - val_loss: 0.2178\n",
      "Epoch 145/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.7703e-07 - val_loss: 0.2178\n",
      "Epoch 146/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.8620e-06 - val_loss: 0.2177\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.1037e-07 - val_loss: 0.2177\n",
      "Epoch 148/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.9258e-06 - val_loss: 0.2177\n",
      "Epoch 149/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 4.0254e-06 - val_loss: 0.2175\n",
      "Epoch 150/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6978e-10 - val_loss: 0.2175\n",
      "Epoch 151/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.1628e-06 - val_loss: 0.2175\n",
      "Epoch 152/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.3301e-07 - val_loss: 0.2175\n",
      "Epoch 153/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.9399e-07 - val_loss: 0.2175\n",
      "Epoch 154/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 7.2272e-06 - val_loss: 0.2175\n",
      "Epoch 155/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4928e-06 - val_loss: 0.2175\n",
      "Epoch 156/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.7599e-07 - val_loss: 0.2174\n",
      "Epoch 157/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.6699e-06 - val_loss: 0.2173\n",
      "Epoch 158/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.1670e-09 - val_loss: 0.2173\n",
      "Epoch 159/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.7664e-07 - val_loss: 0.2173\n",
      "Epoch 160/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 7.7479e-06 - val_loss: 0.2172\n",
      "Epoch 161/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.0497e-09 - val_loss: 0.2172\n",
      "Epoch 162/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.1955e-06 - val_loss: 0.2172\n",
      "Epoch 163/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.8307e-06 - val_loss: 0.2171\n",
      "Epoch 164/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 8.4149e-07 - val_loss: 0.2171\n",
      "Epoch 165/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.7080e-06 - val_loss: 0.2171\n",
      "Epoch 166/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.6108e-06 - val_loss: 0.2170\n",
      "Epoch 167/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.3130e-06 - val_loss: 0.2170\n",
      "Epoch 168/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.1320e-06 - val_loss: 0.2170\n",
      "Epoch 169/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.6626e-06 - val_loss: 0.2169\n",
      "Epoch 170/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.8215e-06 - val_loss: 0.2169\n",
      "Epoch 171/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.5276e-06 - val_loss: 0.2168\n",
      "Epoch 172/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.7828e-06 - val_loss: 0.2168\n",
      "Epoch 173/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.1597e-06 - val_loss: 0.2167\n",
      "Epoch 174/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.4133e-07 - val_loss: 0.2168\n",
      "Epoch 175/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.9161e-06 - val_loss: 0.2168\n",
      "Epoch 176/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.1917e-06 - val_loss: 0.2167\n",
      "Epoch 177/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 4.6503e-07 - val_loss: 0.2167\n",
      "Epoch 178/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.3886e-06 - val_loss: 0.2167\n",
      "Epoch 179/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.0921e-06 - val_loss: 0.2167\n",
      "Epoch 180/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.6966e-06 - val_loss: 0.2166\n",
      "Epoch 181/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 9.3363e-07 - val_loss: 0.2166\n",
      "Epoch 182/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.1967e-06 - val_loss: 0.2166\n",
      "Epoch 183/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 9.1143e-07 - val_loss: 0.2166\n",
      "Epoch 184/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.4500e-06 - val_loss: 0.2165\n",
      "Epoch 185/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.5422e-06 - val_loss: 0.2165\n",
      "Epoch 186/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 9.1552e-07 - val_loss: 0.2164\n",
      "Epoch 187/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.2629e-06 - val_loss: 0.2164\n",
      "Epoch 188/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.8240e-08 - val_loss: 0.2164\n",
      "Epoch 189/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.6361e-07 - val_loss: 0.2164\n",
      "Epoch 190/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.9345e-06 - val_loss: 0.2164\n",
      "Epoch 191/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.3060e-06 - val_loss: 0.2164\n",
      "Epoch 192/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.8702e-06 - val_loss: 0.2163\n",
      "Epoch 193/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.9094e-06 - val_loss: 0.2163\n",
      "Epoch 194/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.5234e-07 - val_loss: 0.2163\n",
      "Epoch 195/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.9850e-06 - val_loss: 0.2163\n",
      "Epoch 196/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.1859e-06 - val_loss: 0.2163\n",
      "Epoch 197/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 6.1858e-08 - val_loss: 0.2163\n",
      "Epoch 198/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.7275e-06 - val_loss: 0.2162\n",
      "Epoch 199/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.0892e-06 - val_loss: 0.2162\n",
      "Epoch 200/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.5163e-06 - val_loss: 0.2161\n",
      "Epoch 201/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.2385e-06 - val_loss: 0.2160\n",
      "Epoch 202/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6724e-06 - val_loss: 0.2160\n",
      "Epoch 203/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.5773e-06 - val_loss: 0.2160\n",
      "Epoch 204/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.1921e-06 - val_loss: 0.2159\n",
      "Epoch 205/300\n",
      "31928/31928 [==============================] - 1s 40us/step - loss: 7.7946e-08 - val_loss: 0.2159\n",
      "Epoch 206/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 3.2560e-06 - val_loss: 0.2159\n",
      "Epoch 207/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 1.5658e-06 - val_loss: 0.2159\n",
      "Epoch 208/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 3.7631e-06 - val_loss: 0.2159\n",
      "Epoch 209/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 7.0934e-07 - val_loss: 0.2159\n",
      "Epoch 210/300\n",
      "31928/31928 [==============================] - 1s 37us/step - loss: 5.4561e-07 - val_loss: 0.2159\n",
      "Epoch 211/300\n",
      "31928/31928 [==============================] - 1s 40us/step - loss: 4.6422e-06 - val_loss: 0.2158\n",
      "Epoch 212/300\n",
      "31928/31928 [==============================] - 1s 40us/step - loss: 8.7948e-07 - val_loss: 0.2158\n",
      "Epoch 213/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 3.3031e-07 - val_loss: 0.2158\n",
      "Epoch 214/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 4.9680e-06 - val_loss: 0.2158\n",
      "Epoch 215/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 4.2291e-07 - val_loss: 0.2158\n",
      "Epoch 216/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.3668e-06 - val_loss: 0.2158\n",
      "Epoch 217/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 1.7466e-06 - val_loss: 0.2159\n",
      "Epoch 218/300\n",
      "31928/31928 [==============================] - 1s 36us/step - loss: 1.7545e-06 - val_loss: 0.2159\n",
      "Epoch 219/300\n",
      "31928/31928 [==============================] - 1s 35us/step - loss: 8.7885e-07 - val_loss: 0.2159\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.4216e-06 - val_loss: 0.2159\n",
      "Epoch 221/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4303e-06 - val_loss: 0.2159\n",
      "Epoch 222/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.4463e-07 - val_loss: 0.2159\n",
      "Epoch 223/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.7677e-06 - val_loss: 0.2159\n",
      "Epoch 224/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.0452e-07 - val_loss: 0.2159\n",
      "Epoch 225/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.8647e-06 - val_loss: 0.2159\n",
      "Epoch 226/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.0136e-07 - val_loss: 0.2159\n",
      "Epoch 227/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.6325e-06 - val_loss: 0.2159\n",
      "Epoch 228/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.9410e-07 - val_loss: 0.2159\n",
      "Epoch 229/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 2.2680e-06 - val_loss: 0.2158\n",
      "Epoch 230/300\n",
      "31928/31928 [==============================] - 1s 34us/step - loss: 1.2724e-06 - val_loss: 0.2160\n",
      "Epoch 231/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.6073e-07 - val_loss: 0.2160\n",
      "Epoch 232/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.1070e-06 - val_loss: 0.2160\n",
      "Epoch 233/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4958e-06 - val_loss: 0.2160\n",
      "Epoch 234/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.7965e-07 - val_loss: 0.2160\n",
      "Epoch 235/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.2131e-06 - val_loss: 0.2160\n",
      "Epoch 236/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.7363e-07 - val_loss: 0.2160\n",
      "Epoch 237/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4297e-06 - val_loss: 0.2160\n",
      "Epoch 238/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.1915e-06 - val_loss: 0.2160\n",
      "Epoch 239/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.6630e-07 - val_loss: 0.2160\n",
      "Epoch 240/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.4252e-06 - val_loss: 0.2161\n",
      "Epoch 241/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.3994e-07 - val_loss: 0.2161\n",
      "Epoch 242/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.8851e-06 - val_loss: 0.2161\n",
      "Epoch 243/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.3417e-06 - val_loss: 0.2161\n",
      "Epoch 244/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.2700e-06 - val_loss: 0.2161\n",
      "Epoch 245/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.5026e-07 - val_loss: 0.2161\n",
      "Epoch 246/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.8708e-06 - val_loss: 0.2161\n",
      "Epoch 247/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.8618e-07 - val_loss: 0.2161\n",
      "Epoch 248/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 3.6167e-06 - val_loss: 0.2161\n",
      "Epoch 249/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.4624e-08 - val_loss: 0.2161\n",
      "Epoch 250/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.4779e-07 - val_loss: 0.2161\n",
      "Epoch 251/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.2929e-06 - val_loss: 0.2161\n",
      "Epoch 252/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.1561e-06 - val_loss: 0.2161\n",
      "Epoch 253/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 8.0384e-07 - val_loss: 0.2161\n",
      "Epoch 254/300\n",
      "31928/31928 [==============================] - 1s 30us/step - loss: 2.7358e-06 - val_loss: 0.2161\n",
      "Epoch 255/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.4327e-07 - val_loss: 0.2161\n",
      "Epoch 256/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.3761e-06 - val_loss: 0.2161\n",
      "Epoch 257/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 4.1181e-06 - val_loss: 0.2161\n",
      "Epoch 258/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.2896e-08 - val_loss: 0.2161\n",
      "Epoch 259/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.0082e-06 - val_loss: 0.2161\n",
      "Epoch 260/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 8.5938e-07 - val_loss: 0.2161\n",
      "Epoch 261/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.1615e-06 - val_loss: 0.2161\n",
      "Epoch 262/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.3779e-06 - val_loss: 0.2161\n",
      "Epoch 263/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4219e-06 - val_loss: 0.2161\n",
      "Epoch 264/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6832e-06 - val_loss: 0.2160\n",
      "Epoch 265/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.2358e-06 - val_loss: 0.2160\n",
      "Epoch 266/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.8838e-07 - val_loss: 0.2160\n",
      "Epoch 267/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 7.1127e-07 - val_loss: 0.2160\n",
      "Epoch 268/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6010e-06 - val_loss: 0.2160\n",
      "Epoch 269/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.3870e-06 - val_loss: 0.2160\n",
      "Epoch 270/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6698e-06 - val_loss: 0.2160\n",
      "Epoch 271/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 5.7833e-07 - val_loss: 0.2160\n",
      "Epoch 272/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.5037e-06 - val_loss: 0.2159\n",
      "Epoch 273/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.2323e-06 - val_loss: 0.2159\n",
      "Epoch 274/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 6.0721e-07 - val_loss: 0.2159\n",
      "Epoch 275/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 8.2555e-07 - val_loss: 0.2159\n",
      "Epoch 276/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.6577e-06 - val_loss: 0.2159\n",
      "Epoch 277/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 8.8979e-07 - val_loss: 0.2159\n",
      "Epoch 278/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.3308e-06 - val_loss: 0.2159\n",
      "Epoch 279/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.6124e-07 - val_loss: 0.2159\n",
      "Epoch 280/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.3871e-06 - val_loss: 0.2159\n",
      "Epoch 281/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.5351e-06 - val_loss: 0.2159\n",
      "Epoch 282/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 2.9196e-07 - val_loss: 0.2159\n",
      "Epoch 283/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.1378e-06 - val_loss: 0.2159\n",
      "Epoch 284/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.6984e-06 - val_loss: 0.2159\n",
      "Epoch 285/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.2589e-06 - val_loss: 0.2159\n",
      "Epoch 286/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4241e-06 - val_loss: 0.2160\n",
      "Epoch 287/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 3.4202e-06 - val_loss: 0.2159\n",
      "Epoch 288/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.0539e-09 - val_loss: 0.2159\n",
      "Epoch 289/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.9424e-06 - val_loss: 0.2160\n",
      "Epoch 290/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.6762e-06 - val_loss: 0.2159\n",
      "Epoch 291/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 7.5542e-07 - val_loss: 0.2159\n",
      "Epoch 292/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4883e-06 - val_loss: 0.2159\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.4608e-06 - val_loss: 0.2159\n",
      "Epoch 294/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 5.9159e-07 - val_loss: 0.2159\n",
      "Epoch 295/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 2.3199e-06 - val_loss: 0.2159\n",
      "Epoch 296/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 6.0206e-07 - val_loss: 0.2159\n",
      "Epoch 297/300\n",
      "31928/31928 [==============================] - 1s 32us/step - loss: 1.2886e-06 - val_loss: 0.2159\n",
      "Epoch 298/300\n",
      "31928/31928 [==============================] - 1s 33us/step - loss: 1.2965e-06 - val_loss: 0.2159\n",
      "Epoch 299/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 4.0938e-07 - val_loss: 0.2159\n",
      "Epoch 300/300\n",
      "31928/31928 [==============================] - 1s 31us/step - loss: 1.7777e-06 - val_loss: 0.2159\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size = 64, epochs =300, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_300 = load_model('./my_model_300.h5')\n",
    "model_500 = load_model('./my_model_500.h5')\n",
    "model_con_300 = load_model('./my_model_con_300.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = model.layers[2].get_weights()[0]\n",
    "#embeddings = model.layers[4].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings.shape\n",
    "# embed = embeddings.T\n",
    "# print(embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_name_dict = {'CGU':\"長庚大學\", 'CMU':\"中國醫藥大學\", 'CYCU':\"中原大學\", 'FCU':\"逢甲大學\", 'KMU':\"高雄醫學大學\", \n",
    "                'NCCU':\"政治大學\", 'NCHU':\"中興大學\", 'NCKU':\"成功大學\", 'NCTU':\"交通大學\", 'NCUE':\"彰化師範大學\", \n",
    "                'NDHU':\"東華大學\", 'NKNU':\"高雄師範大學\", 'NPTU':\"屏東大學\", 'NSYSU':\"中山大學\", 'NTCU':\"臺中教育大學\", \n",
    "                'NTHU':\"清華大學\", 'NTNU':\"臺灣師範大學\", 'NTU':\"臺灣大學\", 'NTUE':\"臺北教育大學\", 'NUTN':\"臺南大學\",\n",
    "                'SCU':\"東吳大學\", 'THU':\"東海大學\", 'TKU':\"淡江大學\", 'TNUA':\"臺北藝術大學\", 'UTAIPEI':\"臺北市立大學\",\n",
    "                'YM':\"陽明大學\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"馬崇堯\", \"周東誼\", \"鬍子元\", \"龍澳天\", \"黃智賢\"]\n",
    "#name = \"周東誼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharEmbed(name):\n",
    "    temp = []\n",
    "    for char in name:\n",
    "        try:\n",
    "            temp.append(dict_char[char])\n",
    "        except:\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "            \n",
    "    if len(temp) == 2:\n",
    "        # k, v = random.choice(list(dict_char.items()))\n",
    "        user_emb = np.mean([temp[0], temp[1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[1]))\n",
    "    elif len(temp) == 3:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    elif len(temp) == 4:\n",
    "        user_emb = np.mean([temp[0], temp[1], temp[2], temp[-1]], axis=0)\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            k, v = random.choice(list(dict_char.items()))\n",
    "            temp.append(v)\n",
    "        user_emb = v\n",
    "        con_emb = np.concatenate((temp[0], temp[1], temp[2]))\n",
    "    \n",
    "    #return user_emb\n",
    "    return con_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = []\n",
    "for name in names:\n",
    "    u = getCharEmbed(name)\n",
    "    us.append(u)\n",
    "sc = np.array(range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "print(us[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict([np.random.random((10, 256)), np.random.randint(0,32, (10))])\n",
    "#np.random.random((10, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'董事': 0,\n",
       " '董事長': 1,\n",
       " '獨立董事': 2,\n",
       " '副董事長': 3,\n",
       " '常務董事': 4,\n",
       " '執行業務股東': 5,\n",
       " 'CGU': 6,\n",
       " 'CMU': 7,\n",
       " 'CYCU': 8,\n",
       " 'FCU': 9,\n",
       " 'KMU': 10,\n",
       " 'NCCU': 11,\n",
       " 'NCHU': 12,\n",
       " 'NCKU': 13,\n",
       " 'NCTU': 14,\n",
       " 'NCUE': 15,\n",
       " 'NDHU': 16,\n",
       " 'NKNU': 17,\n",
       " 'NPTU': 18,\n",
       " 'NSYSU': 19,\n",
       " 'NTCU': 20,\n",
       " 'NTHU': 21,\n",
       " 'NTNU': 22,\n",
       " 'NTU': 23,\n",
       " 'NTUE': 24,\n",
       " 'NUTN': 25,\n",
       " 'SCU': 26,\n",
       " 'THU': 27,\n",
       " 'TKU': 28,\n",
       " 'TNUA': 29,\n",
       " 'UTAIPEI': 30,\n",
       " 'YM': 31}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open ('./dataset/item_index_withEN.pkl', 'rb')\n",
    "item_index_dict = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "item_index_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchool(item_index_dict, score_list):\n",
    "    broad_list = []\n",
    "    for i in range(len(score_list)):\n",
    "        if score_list[i] == 1:\n",
    "            for k, v in item_index_dict.items():\n",
    "                if v == i:\n",
    "                    broad_list.append(k)\n",
    "    return broad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "馬崇堯\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "周東誼\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "鬍子元\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "龍澳天\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "黃智賢\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(us)):\n",
    "    print(names[i])\n",
    "    u = us[i]\n",
    "    score_list = []\n",
    "    for i in range(32):\n",
    "        output = model.predict([[u], [i]])\n",
    "        if output[0][0] > 0.85:\n",
    "            score_list.append(1)\n",
    "        else:\n",
    "            score_list.append(0)\n",
    "    print(score_list)\n",
    "    output_list = getSchool(item_index_dict, score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = getSchool(item_index_dict, score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CGU', 'CMU', 'CYCU', 'FCU', 'KMU', 'NCCU', 'NCHU', 'NCKU', 'NCTU', 'NCUE', 'NDHU', 'NKNU', 'NPTU', 'NSYSU', 'NTCU', 'NTHU', 'NTNU', 'NTU', 'NTUE', 'NUTN', 'SCU', 'THU', 'TKU', 'TNUA', 'UTAIPEI', 'YM']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(output_list)\n",
    "print(len(output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSchool(name):\n",
    "    count = 0\n",
    "    max_score = 0\n",
    "    max_sc_index = 0\n",
    "    print(name)\n",
    "    u = getCharEmbed(name)\n",
    "    score_list = []\n",
    "    ori_score_dict = {}\n",
    "    for i in range(32):\n",
    "        output = model_con_300.predict([[u], [i]])\n",
    "        #output = model.predict([[u], [i]])\n",
    "        ori_score_dict[output[0][0]] = i\n",
    "        if output[0][0] > max_score:\n",
    "            max_score = output[0][0]\n",
    "            max_sc_index = i\n",
    "        if output[0][0] > 0.82:\n",
    "            score_list.append(1)\n",
    "            count += 1\n",
    "        else:\n",
    "            score_list.append(0)\n",
    "    print(\"你有極高機率會上的學校或職位有 :\", count, \"個\")\n",
    "    output_list = getSchool(item_index_dict, score_list)\n",
    "    print(\"你有極高機率會上的學校或職位有 :\", end=\" \")\n",
    "    for out in output_list:\n",
    "        print(sc_name_dict[out], end=\" \")\n",
    "    print(\"\\n\")\n",
    "    ori_score_list = list(ori_score_dict.keys())\n",
    "    ori_score_list.sort(reverse=True)\n",
    "    \n",
    "    print(\"你會上的機率前五高學校或職位有 :\")\n",
    "    # print(ori_score_dict)\n",
    "    # print(list(ori_score_dict.keys()))\n",
    "    for ori in ori_score_list[:5]:\n",
    "        print(list(sc_name_dict.values())[ori_score_dict[ori]-6], end=\", \")\n",
    "    print(\"\\n\")\n",
    "    if count > 0:\n",
    "        print(\"\\n機率最大的是:\", list(sc_name_dict.values())[max_sc_index-6])\n",
    "        #print(max_sc_index)\n",
    "    print(\"\\n\")\n",
    "    max_sc_index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "馬崇堯\n",
      "你有極高機率會上的學校或職位有 : 26 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 中原大學 逢甲大學 高雄醫學大學 政治大學 中興大學 成功大學 交通大學 彰化師範大學 東華大學 高雄師範大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東吳大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "臺南大學, 逢甲大學, 臺中教育大學, 臺灣大學, 臺北教育大學, \n",
      "\n",
      "\n",
      "機率最大的是: 臺南大學\n",
      "\n",
      "\n",
      "周東誼\n",
      "你有極高機率會上的學校或職位有 : 0 個\n",
      "你有極高機率會上的學校或職位有 : \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "高雄師範大學, 清華大學, 臺北藝術大學, 逢甲大學, 中山大學, \n",
      "\n",
      "\n",
      "\n",
      "鬍子元\n",
      "你有極高機率會上的學校或職位有 : 0 個\n",
      "你有極高機率會上的學校或職位有 : \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "臺中教育大學, 中山大學, 長庚大學, 臺北市立大學, 彰化師範大學, \n",
      "\n",
      "\n",
      "\n",
      "龍澳天\n",
      "你有極高機率會上的學校或職位有 : 26 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 中原大學 逢甲大學 高雄醫學大學 政治大學 中興大學 成功大學 交通大學 彰化師範大學 東華大學 高雄師範大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東吳大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "逢甲大學, 臺南大學, 政治大學, 長庚大學, 臺灣大學, \n",
      "\n",
      "\n",
      "機率最大的是: 逢甲大學\n",
      "\n",
      "\n",
      "黃智賢\n",
      "你有極高機率會上的學校或職位有 : 21 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 逢甲大學 高雄醫學大學 政治大學 交通大學 彰化師範大學 東華大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "中山大學, 逢甲大學, 臺灣大學, 臺中教育大學, 臺南大學, \n",
      "\n",
      "\n",
      "機率最大的是: 中山大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    predictSchool(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韓國瑜\n",
      "你有極高機率會上的學校或職位有 : 23 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 逢甲大學 高雄醫學大學 政治大學 中興大學 交通大學 東華大學 高雄師範大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東吳大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "臺南大學, 東海大學, 屏東大學, 政治大學, 臺灣大學, \n",
      "\n",
      "\n",
      "機率最大的是: 臺南大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"韓國瑜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黃崇明\n",
      "你有極高機率會上的學校或職位有 : 15 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 逢甲大學 政治大學 交通大學 東華大學 屏東大學 中山大學 臺中教育大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東海大學 臺北藝術大學 臺北市立大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "臺南大學, 東海大學, 臺灣大學, 臺中教育大學, 逢甲大學, \n",
      "\n",
      "\n",
      "機率最大的是: 臺南大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"黃崇明\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王麒詳\n",
      "你有極高機率會上的學校或職位有 : 26 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 中原大學 逢甲大學 高雄醫學大學 政治大學 中興大學 成功大學 交通大學 彰化師範大學 東華大學 高雄師範大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣師範大學 臺灣大學 臺北教育大學 臺南大學 東吳大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "中山大學, 臺南大學, 臺灣大學, 東華大學, 逢甲大學, \n",
      "\n",
      "\n",
      "機率最大的是: 中山大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"王麒詳\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "趙偉廷\n",
      "你有極高機率會上的學校或職位有 : 0 個\n",
      "你有極高機率會上的學校或職位有 : \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "臺灣大學, 清華大學, 淡江大學, 東華大學, 長庚大學, \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"趙偉廷\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黃仁暐\n",
      "你有極高機率會上的學校或職位有 : 19 個\n",
      "你有極高機率會上的學校或職位有 : 長庚大學 中國醫藥大學 逢甲大學 政治大學 成功大學 彰化師範大學 東華大學 屏東大學 中山大學 臺中教育大學 清華大學 臺灣大學 臺北教育大學 臺南大學 東海大學 淡江大學 臺北藝術大學 臺北市立大學 陽明大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "清華大學, 臺中教育大學, 政治大學, 長庚大學, 臺灣大學, \n",
      "\n",
      "\n",
      "機率最大的是: 清華大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"黃仁暐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吳昭儀\n",
      "你有極高機率會上的學校或職位有 : 5 個\n",
      "你有極高機率會上的學校或職位有 : 東華大學 中山大學 清華大學 臺灣大學 臺南大學 \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "中山大學, 臺灣大學, 清華大學, 東華大學, 臺南大學, \n",
      "\n",
      "\n",
      "機率最大的是: 中山大學\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"吳昭儀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林芊吟\n",
      "你有極高機率會上的學校或職位有 : 0 個\n",
      "你有極高機率會上的學校或職位有 : \n",
      "\n",
      "你會上的機率前五高學校或職位有 :\n",
      "長庚大學, 清華大學, 臺灣大學, 東吳大學, 東華大學, \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictSchool(\"林芊吟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
